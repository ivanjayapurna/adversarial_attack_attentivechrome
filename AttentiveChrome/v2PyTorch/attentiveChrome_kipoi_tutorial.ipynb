{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python tutorial for using Attentive Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install kipoi\n",
    "This can be done by using `pip`. (Ex `pip install kipoi`). Note that you need anaconda or miniconda installed. Refer to https://kipoi.org/docs/#installation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Attentive Chrome in your Python program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import kipoi. Also, we download our example file for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kipoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we want to predict for cell type E005. We first go ahead and create a model for the cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/api/files/2bf982b6-143f-49f6-b9ad-1b1e60f67292/E005_attchrome_avgAUC_model.pt?download=1 to /home/ivan/.kipoi/models/AttentiveChrome/downloaded/model_files/E005/weights/19f61dca439ffcf7bbe44ca15238ff4d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 147k/231k [00:08<00:08, 9.57kB/s] /home/ivan/.local/share/virtualenvs/adversarial-example-genomics-nC5ZSpz9/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "238kB [00:19, 9.57kB/s]                           "
     ]
    }
   ],
   "source": [
    "model = kipoi.get_model(\"AttentiveChrome/E005\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to predict using this model object. First is to predict using the pipeline. This makes prediction for all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0.00B [00:00, ?B/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/record/2640883/files/test.csv?download=1 to /home/ivan/.kipoi/models/AttentiveChrome/downloaded/example_files/input_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 8.19k/22.3k [00:03<00:05, 2.42kB/s]\u001b[A\n",
      " 73%|███████▎  | 16.4k/22.3k [00:03<00:01, 3.23kB/s]\u001b[A\n",
      "\n",
      "24.6kB [00:03, 6.22kB/s]                            \n",
      "1it [00:00, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_dictionary: {'input_file': '/home/ivan/.kipoi/models/AttentiveChrome/downloaded/example_files/input_file'}\n",
      "Number of genes: 10\n",
      "Number of entries: 1000\n",
      "Number of HMs: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dl_dictionary = model.default_dataloader.example_kwargs #This is an example dataloader.\n",
    "print(\"dl_dictionary:\", dl_dictionary)\n",
    "\n",
    "prediction = model.pipeline.predict(dl_dictionary)\n",
    "\n",
    "#If you wish to make prediction on your own dataset, run this code:\n",
    "#prediction = model.pipeline.predict({\"input_file\": \"path to input file\", \"bin_size\": {some integer}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output of prediction is a numpy array containing the output from the final softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result:\n",
      "[[0.6430358 ]\n",
      " [0.04643877]\n",
      " [0.67054904]\n",
      " [0.27077186]\n",
      " [0.6830173 ]\n",
      " [0.5566599 ]\n",
      " [0.18436128]\n",
      " [0.1377412 ]\n",
      " [0.06227126]\n",
      " [0.6060367 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction result:\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to make a prediction is to predict for single batches. We first need to create our dataloader.\n",
    "Then, we can create an iterator of fixed batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes: 10\n",
      "Number of entries: 1000\n",
      "Number of HMs: 7\n"
     ]
    }
   ],
   "source": [
    "dl = model.default_dataloader.init_example()\n",
    "it = dl.batch_iter(batch_size=32) #iterator of batch size 32\n",
    "\n",
    "single_batch = next(it) #this gets us a single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of example, let's make a prediction on the first 10 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction on batch 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6fd811e586dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Making prediction on batch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Making prediction on batch\",i)\n",
    "    prediction = model.predict_on_batch(batch['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
